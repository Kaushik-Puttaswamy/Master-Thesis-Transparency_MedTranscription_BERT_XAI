# Master-Thesis-Transparency_MedTranscription_BERT_XAI
# Overview

The proposed master's thesis aims to enhance the transparency and interpretability of BERT in medical transcription. It will investigate the performance of BERT when fine-tuned on medical-specific data and apply Explainable AI (XAI) techniques to elucidate its decision-making process. The goal is to improve the accuracy and reliability of medical transcription systems, thereby aiding clinical decision-making.

# Research Field

This thesis focuses on evaluating the performance of BERT-based models in medical transcription and exploring Explainable AI (XAI) techniques to enhance model transparency and interpretability. The research aims to provide insights into the model's decision-making process, crucial for building trust in healthcare applications. Integrating BERT with XAI seeks to create a more understandable transcription system, facilitating better clinical judgments and improved patient care.

# Research Problem

The research problem focuses on the lack of transparency and interpretability in deep learning models used for medical transcription. Given the high stakes in healthcare, it is critical to understand how these models make decisions to ensure dependability and safety. Present models' opacity raises concerns about their clinical use. This study aims to develop strategies for elucidating the elements influencing model predictions, thereby enhancing trust and accountability in medical applications.

# Research Objectives

This research evaluates the performance of fine-tuned BERT models in classifying medical transcription data, aiming for state-of-the-art results. It investigates the decision-making process of BERT models using SHAP XAI for interpretability and compares different pre-trained BERT models to identify the highest-performing one. Additionally, it analyzes model errors using word importance metrics to identify and address systematic errors posing potential safety risks in clinical settings.

# Literature Review Summary

Medical text transcription is vital for effective healthcare communication and decision-making. AI-driven methods, including BERT-based models like BioBERT and ClinicalBERT, show promise but often lack interpretability. Integrating XAI approaches like SHAP and LIME is still developing, and systematic comparisons across medical specialties are needed. Unstructured clinical notes and limited annotated datasets present challenges, requiring domain-specific preprocessing and collaboration with clinical professionals to improve AI model accuracy and practicality.

# Research Gap

Despite advancements with BERT-based models in medical text transcription, there is a lack of systematic studies comparing their performance across medical disciplines. Current research often neglects interpretability and transparency, which are essential for clinical adoption. The integration of XAI methods like SHAP and LIME is still in its early stages, and comprehensive comparisons between traditional and transformer-based models using standardized datasets are missing. Additionally, the challenges of unstructured clinical notes and limited annotated datasets necessitate improved preprocessing and data augmentation techniques.

# Research Methodology


